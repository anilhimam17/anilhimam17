# Long Term Sprint üèÉ

## The Grind üõ£Ô∏è - Since 15th Aug, 2025
### Month - 1 (Foundations)
- [ ] Week 1-2
    - DSA Completion in two weeks with C++ and Python Implementations Key areas of focus: Trees, Graphs and Dynamic Programming `Skipped for now, Too Boring will apply JIT Learning`
    - Blaze through the PyTorch Official Tutorials `Lets Gooo ‚úÖ`
- [ ] Week 3-4
    - Work on Math
        1. Linear Algebra: MIT 18.06 (Gilbert Strang) - Focus on eigenvectors, SVD, matrix decompositions
        2. Calculus: Khan Academy + 3Blue1Brown's Essence of Calculus
        3. Statistics & Probability:  "Think Stats" by Allen Downey    Stanford CS109: Probability for Computer Scientists
        4. Optimization: Boyd & Vandenberghe "Convex Optimization" (first 4 chapters)
    - Work on the Let's build the GPT video

### Month - 2 (Catchup to the Bleeding Edge)
- [ ] Week 1-2
    - Work on Completing the knowledge of the Hands on Machine Learning with ScikitLearn and TensorFlow.
    - Extend the knowledge of the API from TensorFlow to PyTorch.
- [ ] Week 3-4
    - Burst through alot of the `Ground Breaking` Research papers.
    - Transformer Revolution
        - "Attention Is All You Need" - Implement from scratch in PyTorch
        - Multi-head attention mechanisms - Build intuitive understanding
        - Position encodings - Sinusoidal vs learned embeddings
        - Hands-on: Build a mini-GPT from scratch
    - Vision Transformers & Beyond
        - Vision Transformers (ViTs) - "An Image is Worth 16x16 Words"
        - CLIP - Connecting text and images
        - DALL-E 2 & Stable Diffusion architecture understanding
        - Hands-on: Implement ViT for image classification
    
### Month - 3 (Catchup to the Bleeding Edge Continued)
- Burst through alot of the `Ground Breaking` Research papers.
    - [ ] Week 1: Advanced Attention Mechanisms
        - Cross-attention, Self-attention variations
        - Efficient attention: Linformer, Performer, Longformer
        - Sparse attention patterns
        - Flash Attention for memory efficiency
    - [ ] Week 2: Modern Architectures
        - Mamba/State Space Models - Linear scaling attention alternative
        - Mixture of Experts (MoE) architectures
        - Retrieval Augmented Generation (RAG) deep dive
        - Hands-on: Build a RAG system with advanced retrieval strategies
- Generative Models & Diffusion
    - [ ] Week 3-4: 
        - Diffusion Models Mastery
        - DDPM - "Denoising Diffusion Probabilistic Models"
        - DDIM - Faster sampling strategies
        - Score-based generative models
        - Hands-on: Train a diffusion model on custom dataset
    - [ ] Flex Time: 
        - Advanced Generative Techniques
        - VAE-GAN hybrids and Œ≤-VAE
        - Flow-based models (RealNVP, Glow)
        - Energy-based models
        - Hands-on: Compare different generative approaches

### Month 4: Reinforcement Learning Revolution
- [ ] Week 1: RL Fundamentals Refresh
    - PPO, A3C, SAC - Modern policy gradient methods
    - Rainbow DQN and value-based improvements
    - Multi-agent RL basics 
- [ ] Week 2-3: RL for AI Systems
    - RLHF (Reinforcement Learning from Human Feedback)
    - Constitutional AI approaches RLAIF (AI Feedback) methodologies
    - Hands-on: Implement RLHF pipeline for text generation
- [ ] Week 4: Advanced RL 
    - Model-based RL (MBRL)
    - Meta-learning in RL
    - Offline RL techniques

### Month 5: Systems & HPC | Performance Optimisation
- [ ] Week 1-2: GPU Programming Mastery 
    - CUDA fundamentals: Memory hierarchies, thread blocks, warps
    - Kernel optimization: Occupancy, memory coalescing
    - cuDNN and cuBLAS integration
    - Practice on LeetGPU: Start with basic kernels
- [ ] Week 2-3: Distributed Training
    - Data parallelism: DistributedDataParallel (DDP)
    - Model parallelism: Pipeline and tensor parallelism
    - Mixed precision training: FP16, BF16 optimization
    - Gradient accumulation and gradient checkpointing
- [ ] Week 4: Advanced Optimization
    - Quantization: INT8, INT4 techniques
    - Pruning strategies: Structured and unstructured
    - Knowledge distillation for model compression
    - TensorRT and ONNX deployment optimization

### Month 6: MLOps & Production Systems
- [ ] Week 1-2: Orchestration & Workflow Management
    - Apache Airflow for ML pipelines
    - Kubeflow for Kubernetes-native ML
    - MLflow for experiment tracking and model registry
    - DVC for data and model versioning
- [ ] Week 3-4: 
    - Observability & Monitoring
    - Weights & Biases advanced features
    - TensorBoard profiling and debugging
    - Prometheus + Grafana for production monitoring
    - Model drift detection and data quality monitoring

### Month 7: Advanced Python & Systems Programming
- [ ] Python Mastery Completion:
    - Asyncio and concurrency: async/await patterns, event loops
    - Multiprocessing: Process pools, shared memory, IPC
    - Advanced NumPy: Broadcasting, memory layouts, optimization
    - JAX: Functional programming for ML, JIT compilation
    - Ray: Distributed computing and hyperparameter tuning
- [ ] Essential AI/ML Python Stack:
    - Transformers (HuggingFace): Advanced usage patterns
    - Datasets: Efficient data loading and processing
    - Accelerate: Multi-GPU and TPU training
    - DeepSpeed: Large model training optimization
    - Hydra: Configuration management for experiments
    - Optuna: Advanced hyperparameter optimization
- [ ] Systems Programming:
    - Memory profiling: py-spy, memory_profiler, tracemalloc
    - Performance optimization: Cython, Numba integration
    - Container orchestration: Docker, Kubernetes for ML
    - Cloud optimization: AWS SageMaker, GCP Vertex AI, Azure ML

## The Reasearch SuperPower ü¶∏
- Objective: Turn your prototype into a publishable paper.
    - Read at least 50 papers from the last two years during the course of the **LongTerm Sprint** and more as I build towards my first research paper.
    - Baseline & Ablate: Compare my method against existing baselines and conduct ablation studies.
- Research Execution Framework:
    1. Hypothesis Formation: Clear, testable research questions
    2. Experimental Design: Proper baselines, ablation studies
    3. Implementation: Clean, reproducible code
    4. Evaluation: Comprehensive metrics and analysis
    5. Documentation: Detailed logs and version control
